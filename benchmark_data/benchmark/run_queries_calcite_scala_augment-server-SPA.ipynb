{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e4175a-8e11-4ab2-b5f2-af33b04201ab",
   "metadata": {},
   "source": [
    "# Running queries using JSONS produced by Scala code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe4fae-04cc-4b9c-beb7-510662a4660d",
   "metadata": {},
   "source": [
    "Install and import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fa7d7-3b51-416f-b221-1369637b4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6645fd-f261-4197-ab65-bac11ae4cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import multiprocessing\n",
    "import signal\n",
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from py4j.protocol import Py4JJavaError, Py4JError\n",
    "import psutil\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c950d3-619f-4c9c-9eac-0853eb8242d4",
   "metadata": {},
   "source": [
    "Function for running one query. This means\n",
    "*  run the original query 5 times (after one initial run, which we do not use)\n",
    "*  run the rewritten queries 5 times (after one initial run, which we do not use) and drop the created tables each time\n",
    "*  take the runtimes and calculate mean, median and standard deviation of time for either the original or rewritten query\n",
    "*  compare the runtimes between the original query, the rewritten query and the rewritten query + the rewriting time (how long the Scala took)\n",
    "*  save everything in a csv output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9533a-d16e-443e-ab3d-3d9e39229a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "def create_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .master(f'local[{SPARK_CORES}]') \\\n",
    "        .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",False) \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# import the database into Spark from PostgreSQL\n",
    "def import_db(spark, dbname):\n",
    "\n",
    "    if dbname == \"JOB\":\n",
    "        dbname = \"imdb\"\n",
    "    else:\n",
    "        dbname = dbname.lower()\n",
    "    \n",
    "    username = dbname\n",
    "    password = dbname\n",
    "    dbname = dbname\n",
    "\n",
    "    df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://postgres:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "    for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c63c9c-bc9d-45d1-85ab-5ea66178b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for handling TO and cancelling those queries in case of a TO\n",
    "def measure_resource_usage(resource_usage):\n",
    "    t = threading.current_thread()\n",
    "    secs = 0\n",
    "    while getattr(t, \"do_run\", True):\n",
    "        resource_usage.append(get_resource_usage(secs))\n",
    "        #print(\"resource usage: \" + str(resource_usage))\n",
    "        secs += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "def get_resource_usage(t):\n",
    "    return {\n",
    "        'time': t,\n",
    "        'memory': psutil.virtual_memory(),\n",
    "        'cpu': psutil.cpu_percent(interval=None, percpu=True),\n",
    "        'cpu_total': psutil.cpu_percent(interval=None, percpu=False)\n",
    "    }\n",
    "    \n",
    "def cancel_query(spark, seconds, group_id):\n",
    "    time.sleep(seconds)\n",
    "    print(\"cancelling jobs with id \" + group_id)\n",
    "    print(spark.sparkContext.cancelJobGroup(group_id))\n",
    "    print(\"cancelled job\")\n",
    "\n",
    "def cancel_query_after(spark, seconds):\n",
    "    group_id = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(16)) #random id\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    threading.Thread(target=cancel_query, args=(spark, seconds, group_id,)).start()\n",
    "    return group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66996ed5-ee53-41fe-987a-62dc1630ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run the query 6 times checking for TO and calculate and saving all values\n",
    "def run_query(benchmark, query, spark):\n",
    "    print(benchmark, query)\n",
    "    file_path = f'rewritten/{benchmark}_{query}_output.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # get the original and rewritten query\n",
    "    original_query = json_data[\"original_query\"]\n",
    "    rewritten_query_list = json_data[\"rewritten_query\"]\n",
    "    rewriting_time = json_data[\"time\"]\n",
    "\n",
    "    # change the queries such that they can be executed in SparkSQL (without changing the output)\n",
    "    rewritten_query_list_spark = [rewritten_query.replace(\" UNLOGGED TABLE \", \" VIEW \")\n",
    "                                                 .replace(\"TIMESTAMP(0)\", \"TIMESTAMP\")\n",
    "                                                 .replace(\"$\", \"_\")\n",
    "                                                 .replace(\"CREATE VIEW\", \"CREATE TEMPORARY VIEW\")\n",
    "                                  for rewritten_query in rewritten_query_list]\n",
    "\n",
    "    # get the drop queries\n",
    "    file_path_drop = f'rewritten/{benchmark}_{query}_drop.json'\n",
    "    with open(file_path_drop, 'r') as file:\n",
    "        json_drop = json.load(file)\n",
    "    drop_query_list = json_drop[\"rewritten_query\"]\n",
    "\n",
    "    drop_query_list_spark = [drop_query.lower()\n",
    "                                       .replace(\"drop view\", \"drop view if exists\")\n",
    "                                       .replace(\"drop table\", \"drop table if exists\")\n",
    "                             for drop_query in drop_query_list]\n",
    "\n",
    "    for drop_query in drop_query_list_spark:\n",
    "            drop = spark.sql(drop_query)\n",
    "            drop.show()\n",
    "\n",
    "    timeout_flag_orig = True\n",
    "    timeout_flag_rewr = True\n",
    "\n",
    "    # the first run is just a warm up run and to check for the time out\n",
    "    try:\n",
    "        spark.sparkContext._jvm.System.gc()\n",
    "\n",
    "        start_time = time.time()\n",
    "        resource_usage = []\n",
    "        measure_thread = threading.Thread(target=measure_resource_usage, args=(resource_usage, ))\n",
    "        measure_thread.start()\n",
    "    \n",
    "        group_id = cancel_query_after(spark, TIMEOUT)\n",
    "        result = spark.sql(original_query)\n",
    "        result.show()\n",
    "        end_time = time.time()\n",
    "        print(end_time-start_time)\n",
    "    \n",
    "        measure_thread.do_run = False\n",
    "        timeout_flag_orig = False\n",
    "    except Py4JError as e:\n",
    "        print('timeout or error orig: ' + str(e))\n",
    "        \n",
    "    try:\n",
    "        spark.sparkContext._jvm.System.gc()\n",
    "        resource_usage = []\n",
    "        measure_thread = threading.Thread(target=measure_resource_usage, args=(resource_usage, ))\n",
    "        measure_thread.start()\n",
    "\n",
    "        group_id = cancel_query_after(spark, TIMEOUT)\n",
    "        for rewritten_query in rewritten_query_list_spark:\n",
    "            if rewritten_query.startswith(\"SELECT\"):\n",
    "                result1 = spark.sql(rewritten_query)\n",
    "                result1.show()\n",
    "            else:\n",
    "                result2 = spark.sql(rewritten_query)\n",
    "                result2.show()\n",
    "\n",
    "        measure_thread.do_run = False\n",
    "        timeout_flag_rewr = False\n",
    "\n",
    "        for drop_query in drop_query_list_spark:\n",
    "            drop = spark.sql(drop_query)\n",
    "            drop.show()\n",
    "        \n",
    "    except Py4JError as e:\n",
    "        print('timeout or error rewr: ' + str(e))\n",
    "\n",
    "    print(timeout_flag_orig, timeout_flag_rewr)\n",
    "    # original and rewritten query are TOs\n",
    "    if timeout_flag_orig and timeout_flag_rewr:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        orig_or_rewr_mean = \"-\"\n",
    "        orig_or_rewr_or_equal = \"-\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"-\"\n",
    "\n",
    "    # original query is a TO and the rewritten not\n",
    "    elif timeout_flag_orig:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        list_rewritten = []\n",
    "        print(\"rewritten\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "            try:\n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten = time.time()\n",
    "                for rewritten_query in rewritten_query_list_spark:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten = time.time()\n",
    "                rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "                list_rewritten.append(rewritten_time)\n",
    "            except Exception as e:\n",
    "                list_rewritten.append(\"-\")\n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list_spark:\n",
    "                drop = spark.sql(drop_query)\n",
    "                drop.show()\n",
    "        list_rewritten_filtered = [x for x in list_rewritten if x != \"-\"]\n",
    "        rewr_mean = np.mean(list_rewritten_filtered)\n",
    "        rewr_med = np.median(list_rewritten_filtered)\n",
    "        rewr_std = np.std(list_rewritten_filtered)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "\n",
    "        orig_or_rewr_mean = \"rewr\"\n",
    "        orig_or_rewr_or_equal = \"rewr\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "\n",
    "    # rewritten query is a TO and the original not\n",
    "    elif timeout_flag_rewr:\n",
    "        list_original = []\n",
    "        print(\"orig\")\n",
    "        for i in range(5):\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "            # execute the original query\n",
    "            try:\n",
    "                start_time_original = time.time()\n",
    "                exec = spark.sql(original_query)\n",
    "                exec.show()\n",
    "                end_time_original = time.time()\n",
    "                original_time = end_time_original - start_time_original\n",
    "                list_original.append(original_time)\n",
    "            except Exception as e:\n",
    "                list_original.append(\"-\")\n",
    "        list_original_filtered = [x for x in list_original if x != \"-\"]\n",
    "        orig_mean = np.mean(list_original_filtered)\n",
    "        orig_med = np.median(list_original_filtered)\n",
    "        orig_std = np.std(list_original_filtered)\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        orig_or_rewr_mean = \"orig\"\n",
    "        orig_or_rewr_or_equal = \"orig\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"orig\"\n",
    "\n",
    "    # both queries are no TOs\n",
    "    else:\n",
    "        #print(result, result1)\n",
    "        list_original = []\n",
    "        list_rewritten = []\n",
    "        # take times for 5 runs (run 2-6) for the original query and the rewritten query\n",
    "        print(\"orig+rewr\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "            # execute the original query\n",
    "            try: \n",
    "                start_time_original = time.time()\n",
    "                exec = spark.sql(original_query)\n",
    "                exec.show()\n",
    "                end_time_original = time.time()\n",
    "                original_time = end_time_original - start_time_original\n",
    "                list_original.append(original_time)\n",
    "            except Exception as e:\n",
    "                list_original.append(\"-\")\n",
    "\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "            # execute the rewritten query\n",
    "            try:\n",
    "                start_time_rewritten = time.time()\n",
    "                for rewritten_query in rewritten_query_list_spark:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten = time.time()\n",
    "                rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "                list_rewritten.append(rewritten_time)\n",
    "            except Exception as e:\n",
    "                list_rewritten.append(\"-\")\n",
    "            \n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list_spark:\n",
    "                drop = spark.sql(drop_query)\n",
    "                drop.show()\n",
    "\n",
    "        list_original_filtered = [x for x in list_original if x != \"-\"]\n",
    "        list_rewritten_filtered = [x for x in list_rewritten if x != \"-\"]\n",
    "        orig_mean = np.mean(list_original_filtered)\n",
    "        orig_med = np.median(list_original_filtered)\n",
    "        orig_std = np.std(list_original_filtered)\n",
    "        rewr_mean = np.mean(list_rewritten_filtered)\n",
    "        rewr_med = np.median(list_rewritten_filtered)\n",
    "        rewr_std = np.std(list_rewritten_filtered)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "        if orig_mean > rewr_mean:\n",
    "            orig_or_rewr_mean = \"rewr\"\n",
    "        else:\n",
    "            orig_or_rewr_mean = \"orig\"\n",
    "        if abs(rewr_mean-orig_mean) < 0.05:\n",
    "            orig_or_rewr_or_equal = \"equal 0.05\"\n",
    "        else:\n",
    "            orig_or_rewr_or_equal = orig_or_rewr_mean\n",
    "        if orig_mean > rewr_mean_plus_rewr:\n",
    "            orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "        else:\n",
    "            orig_or_rewr_plus_rewr_mean = \"orig\"\n",
    "            \n",
    "    list_output = [benchmark, query] + [orig_mean, rewr_mean, rewr_mean_plus_rewr, orig_or_rewr_mean, orig_or_rewr_or_equal, \\\n",
    "                                        orig_or_rewr_plus_rewr_mean, rewriting_time] + \\\n",
    "                    list_original + [orig_med, orig_std] + list_rewritten + [rewr_med, rewr_std, rewr_med_plus_rewr]\n",
    "    #print(list_output)\n",
    "    file_path = \"results/SPA_Scala_comparison_TO_augment_server.csv\"\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000f324-b53b-486e-b554-643d5fb9cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SPARK_MEMORY = 120\n",
    "SPARK_CORES = 4\n",
    "DBHOST = 'postgres'\n",
    "TIMEOUT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b709d2-6388-4456-991e-a987d9d7bf6b",
   "metadata": {},
   "source": [
    "Create the output csv with the header. We add the running times for each query then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15345ee0-b848-4421-bfe2-8e74b3aa775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/SPA_Scala_comparison_TO_augment_server.csv\"\n",
    "\n",
    "names = [\"bench\", \"query\", \"orig mean\", \"rewr mean\", \"rewr mean+rewr\", \"orig/rewr(mean)\", \"orig/rewr/equal\", \"orig/rewr+rewr(mean)\", \"rewriting\", \n",
    "         \"orig 1\", \"orig 2\", \"orig 3\", \"orig 4\", \"orig 5\", \"orig med\", \"orig_std\", \"rewr 1\", \"rewr 2\", \"rewr 3\", \"rewr 4\", \"rewr 5\", \"rewr med\", \n",
    "         \"rewr_std\", \"rewr med+rewr\", ]\n",
    "\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434e62b-fca5-4eb4-a260-ee5c5591f617",
   "metadata": {},
   "source": [
    "Connect to Spark for each dataset and execute all queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896e8fd-1dcf-44e7-abb6-0fc8ee9f61ad",
   "metadata": {},
   "source": [
    "## STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70cbfb-802d-403b-94c8-9bf9e634e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"STATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c56465-ff3a-4cd9-8512-38189c37d0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('STATS')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c61cf-476a-4966-a227-21a756650f85",
   "metadata": {},
   "source": [
    "### SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8768f-5419-44b9-87fd-69beb2004d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"SNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0b916-8adc-4078-a751-9084a3066a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc3cb4-07a3-4e77-a8f0-e4142c5b3a74",
   "metadata": {},
   "source": [
    "### JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7789b70-2122-4726-9492-1a7b54cbf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"JOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067b0d9-6c73-4590-a8b3-aaf883a883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('JOB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e89aa0-2e25-46d7-90e0-300f131e53ee",
   "metadata": {},
   "source": [
    "### LSQB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4351a-523f-47b9-ad4e-b978a33e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"LSQB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc16d52-b608-48ba-a9d0-11f71ae21c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('LSQB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665d6b1-e165-43a5-ad64-0c92fcb6e92b",
   "metadata": {},
   "source": [
    "### HETIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d09b4-abf3-43c8-b24d-24af457247ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"HETIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06cc75-585e-4848-8826-e35a80c888cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('HETIO')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
