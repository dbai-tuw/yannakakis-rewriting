{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e4175a-8e11-4ab2-b5f2-af33b04201ab",
   "metadata": {},
   "source": [
    "# Running queries using JSONS produced by Scala code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe4fae-04cc-4b9c-beb7-510662a4660d",
   "metadata": {},
   "source": [
    "Install and import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fa7d7-3b51-416f-b221-1369637b4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install duckdb\n",
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6645fd-f261-4197-ab65-bac11ae4cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import csv\n",
    "import multiprocessing\n",
    "import signal\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c950d3-619f-4c9c-9eac-0853eb8242d4",
   "metadata": {},
   "source": [
    "Function for running one query. This means\n",
    "*  run the original query 5 times (after one initial run, which we do not use)\n",
    "*  run the rewritten queries 5 times (after one initial run, which we do not use) and drop the created tables each time\n",
    "*  take the runtimes and calculate mean, median and standard deviation of time for either the original or rewritten query\n",
    "*  compare the runtimes between the original query, the rewritten query and the rewritten query + the rewriting time (how long the Scala took)\n",
    "*  save everything in a csv output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33ec17-289d-4a51-97d7-3044afcd9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to handle the timeouts\n",
    "def handler_orig(signum, frame):\n",
    "    global timeout_flag_orig\n",
    "    timeout_flag_orig = True\n",
    "    raise Exception(\"Query execution of the original query > 100s\")\n",
    "\n",
    "def handler_rewr(signum, frame):\n",
    "    global timeout_flag_rewr\n",
    "    timeout_flag_rewr = True\n",
    "    raise Exception(\"Query execution of the rewritten query > 100s\")\n",
    "\n",
    "# function to run the query 6 times checking for TO and calculate and saving all values\n",
    "def run_query(benchmark, query):\n",
    "    print(benchmark, query)\n",
    "    file_path = f'rewritten/{benchmark}_{query}_output.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # get the original and rewritten query\n",
    "    original_query = json_data[\"original_query\"]\n",
    "    rewritten_query_list = json_data[\"rewritten_query\"]\n",
    "    rewriting_time = json_data[\"time\"]\n",
    "\n",
    "    # change the queries such that they can be executed in DuckDB (without changing the output)\n",
    "    rewritten_query_list_ddb = [rewritten_query.replace(\"TIMESTAMP(0)\", \"TIMESTAMP\").lower()\n",
    "                                  for rewritten_query in rewritten_query_list]\n",
    "\n",
    "    # get the drop queries\n",
    "    file_path_drop = f'rewritten/{benchmark}_{query}_drop.json'\n",
    "    with open(file_path_drop, 'r') as file:\n",
    "        json_drop = json.load(file)\n",
    "    drop_query_list = json_drop[\"rewritten_query\"]\n",
    "\n",
    "    drop_query_list_ddb = [drop_query.lower() for drop_query in drop_query_list]\n",
    "\n",
    "    # connect to DuckDB\n",
    "    database = benchmark.lower() + \"/\" + benchmark.lower() + \".duckdb\"\n",
    "    conn = duckdb.connect(database=database)\n",
    "\n",
    "    # if the evaluation takes longer than 100sec then break it\n",
    "    global timeout_flag_orig\n",
    "    global timeout_flag_rewr\n",
    "    timeout_flag_orig = False\n",
    "    timeout_flag_rewr = False\n",
    "\n",
    "    print(\"original1\")\n",
    "    # the first run is just a warm up run and to check for the time out\n",
    "    signal.signal(signal.SIGALRM, handler_orig) \n",
    "    signal.alarm(100) #TO at 100 sec, can be changed\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE \" + benchmark.lower() + \"_DDB\")\n",
    "        cur.execute(original_query)\n",
    "        result = cur.fetchall()\n",
    "    except Exception as exc: \n",
    "        print(exc)\n",
    "    signal.alarm(0) \n",
    "\n",
    "    print(\"rewritten1\")\n",
    "    signal.signal(signal.SIGALRM, handler_rewr) \n",
    "    signal.alarm(100) \n",
    "    try:\n",
    "        for rewritten_query in rewritten_query_list_ddb:\n",
    "            cur.execute(rewritten_query)\n",
    "            if rewritten_query.startswith(\"select\"):\n",
    "                result1 = cur.fetchall()\n",
    "        for drop_query in drop_query_list_ddb:\n",
    "            cur.execute(drop_query)\n",
    "    except Exception as exc: \n",
    "        print(exc)\n",
    "    signal.alarm(0)\n",
    "\n",
    "    print(timeout_flag_orig, timeout_flag_rewr)\n",
    "    # original and rewritten query are TOs\n",
    "    if timeout_flag_orig and timeout_flag_rewr:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        orig_or_rewr_mean = \"-\"\n",
    "        orig_or_rewr_or_equal = \"-\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"-\"\n",
    "\n",
    "        for drop_query in drop_query_list_ddb:\n",
    "            drop_query = drop_query.replace(\"drop view\", \"drop view if exists\").replace(\"drop table\", \"drop table if exists\")\n",
    "            #print(drop_query)\n",
    "            cur.execute(drop_query)\n",
    "\n",
    "    # original query is a TO and the rewritten not\n",
    "    elif timeout_flag_orig:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        list_rewritten = []\n",
    "        print(\"rewritten\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            # execute the rewritten query\n",
    "            start_time_rewritten = time.time()\n",
    "            for rewritten_query in rewritten_query_list_ddb:\n",
    "                cur.execute(rewritten_query)\n",
    "            end_time_rewritten = time.time()\n",
    "            rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "            list_rewritten.append(rewritten_time)\n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list_ddb:\n",
    "                cur.execute(drop_query)\n",
    "        rewr_mean = np.mean(list_rewritten)\n",
    "        rewr_med = np.median(list_rewritten)\n",
    "        rewr_std = np.std(list_rewritten)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "\n",
    "        orig_or_rewr_mean = \"rewr\"\n",
    "        orig_or_rewr_or_equal = \"rewr\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "\n",
    "    # rewritten query is a TO and the original not\n",
    "    elif timeout_flag_rewr:\n",
    "        list_original = []\n",
    "        print(\"orig\")\n",
    "        for i in range(5):\n",
    "            # execute the original query\n",
    "            start_time_original = time.time()\n",
    "            cur.execute(original_query)\n",
    "            end_time_original = time.time()\n",
    "            original_time = end_time_original - start_time_original\n",
    "            list_original.append(original_time)\n",
    "        orig_mean = np.mean(list_original)\n",
    "        orig_med = np.median(list_original)\n",
    "        orig_std = np.std(list_original)\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        orig_or_rewr_mean = \"orig\"\n",
    "        orig_or_rewr_or_equal = \"orig\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"orig\"\n",
    "\n",
    "        for drop_query in drop_query_list_ddb:\n",
    "            drop_query = drop_query.replace(\"drop view\", \"drop view if exists\").replace(\"drop table\", \"drop table if exists\")\n",
    "            #print(drop_query)\n",
    "            cur.execute(drop_query)\n",
    "\n",
    "    # both queries are no TOs\n",
    "    else:\n",
    "        print(result, result1)\n",
    "        list_original = []\n",
    "        list_rewritten = []\n",
    "        # take times for 5 runs (run 2-6) for the original query and the rewritten query\n",
    "        print(\"orig+rewr\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            # execute the original query\n",
    "            start_time_original = time.time()\n",
    "            cur.execute(original_query)\n",
    "            end_time_original = time.time()\n",
    "            original_time = end_time_original - start_time_original\n",
    "            list_original.append(original_time)\n",
    "        \n",
    "            # execute the rewritten query\n",
    "            start_time_rewritten = time.time()\n",
    "            for rewritten_query in rewritten_query_list_ddb:\n",
    "                cur.execute(rewritten_query)\n",
    "            end_time_rewritten = time.time()\n",
    "            rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "            list_rewritten.append(rewritten_time)\n",
    "            \n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list_ddb:\n",
    "                cur.execute(drop_query)\n",
    "            \n",
    "        orig_mean = np.mean(list_original)\n",
    "        orig_med = np.median(list_original)\n",
    "        orig_std = np.std(list_original)\n",
    "        rewr_mean = np.mean(list_rewritten)\n",
    "        rewr_med = np.median(list_rewritten)\n",
    "        rewr_std = np.std(list_rewritten)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "        if orig_mean > rewr_mean:\n",
    "            orig_or_rewr_mean = \"rewr\"\n",
    "        else:\n",
    "            orig_or_rewr_mean = \"orig\"\n",
    "        if abs(rewr_mean-orig_mean) < 0.05:\n",
    "            orig_or_rewr_or_equal = \"equal 0.05\"\n",
    "        else:\n",
    "            orig_or_rewr_or_equal = orig_or_rewr_mean\n",
    "        if orig_mean > rewr_mean_plus_rewr:\n",
    "            orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "        else:\n",
    "            orig_or_rewr_plus_rewr_mean = \"orig\"\n",
    "            \n",
    "    list_output = [benchmark, query] + [orig_mean, rewr_mean, rewr_mean_plus_rewr, orig_or_rewr_mean, orig_or_rewr_or_equal, \\\n",
    "                                        orig_or_rewr_plus_rewr_mean, rewriting_time] + \\\n",
    "                    list_original + [orig_med, orig_std] + list_rewritten + [rewr_med, rewr_std, rewr_med_plus_rewr]\n",
    "    #print(list_output)\n",
    "    file_path = \"results/DDB_Scala_comparison_TO_augment_server.csv\"\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52220abe-c479-48d0-af30-69d0a1781e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the original query is a timeout for sure, then only run for the rewritten one\n",
    "def run_query_rewritten(benchmark, query):\n",
    "    print(benchmark, query)\n",
    "    file_path = f'rewritten/{benchmark}_{query}_output.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    \n",
    "    original_query = json_data[\"original_query\"]\n",
    "    rewritten_query_list = json_data[\"rewritten_query\"]\n",
    "    rewriting_time = json_data[\"time\"]\n",
    "\n",
    "    rewritten_query_list_ddb = [rewritten_query.replace(\"TIMESTAMP(0)\", \"TIMESTAMP\").lower()\n",
    "                                  for rewritten_query in rewritten_query_list]\n",
    "\n",
    "    file_path_drop = f'rewritten/{benchmark}_{query}_drop.json'\n",
    "    with open(file_path_drop, 'r') as file:\n",
    "        json_drop = json.load(file)\n",
    "    drop_query_list = json_drop[\"rewritten_query\"]\n",
    "\n",
    "    drop_query_list_ddb = [drop_query.lower() for drop_query in drop_query_list]\n",
    "\n",
    "    database = benchmark.lower() + \"/\" + benchmark.lower() + \".duckdb\"\n",
    "    conn = duckdb.connect(database=database)\n",
    "\n",
    "    # if the evaluation takes longer than 30min then break it\n",
    "    global timeout_flag_rewr\n",
    "    timeout_flag_rewr = False\n",
    "\n",
    "    signal.signal(signal.SIGALRM, handler_rewr) \n",
    "    signal.alarm(100) \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE \" + benchmark.lower() + \"_DDB\")\n",
    "        for rewritten_query in rewritten_query_list_ddb:\n",
    "            cur.execute(rewritten_query)\n",
    "            if rewritten_query.startswith(\"select\"):\n",
    "                result1 = cur.fetchall()\n",
    "        for drop_query in drop_query_list_ddb:\n",
    "            cur.execute(drop_query)\n",
    "    except Exception as exc: \n",
    "        print(exc)\n",
    "    signal.alarm(0)\n",
    "\n",
    "    # rewritten query is a TO too\n",
    "    if timeout_flag_rewr:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        orig_or_rewr_mean = \"-\"\n",
    "        orig_or_rewr_or_equal = \"-\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"-\"\n",
    "\n",
    "        for drop_query in drop_query_list_ddb:\n",
    "            drop_query = drop_query.replace(\"drop view\", \"drop view if exists\").replace(\"drop table\", \"drop table if exists\")\n",
    "            #print(drop_query)\n",
    "            cur.execute(drop_query)\n",
    "    \n",
    "\n",
    "    # rewritten query is not a TO\n",
    "    else:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        list_rewritten = []\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            # execute the rewritten query\n",
    "            start_time_rewritten = time.time()\n",
    "            for rewritten_query in rewritten_query_list_ddb:\n",
    "                cur.execute(rewritten_query)\n",
    "            end_time_rewritten = time.time()\n",
    "            rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "            list_rewritten.append(rewritten_time)\n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list_ddb:\n",
    "                cur.execute(drop_query)\n",
    "        rewr_mean = np.mean(list_rewritten)\n",
    "        rewr_med = np.median(list_rewritten)\n",
    "        rewr_std = np.std(list_rewritten)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "\n",
    "        orig_or_rewr_mean = \"rewr\"\n",
    "        orig_or_rewr_or_equal = \"rewr\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "\n",
    "    orig_mean = \"TO\"\n",
    "    orig_med = \"TO\"\n",
    "    orig_std = \"-\"\n",
    "    list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "            \n",
    "    list_output = [benchmark, query] + [orig_mean, rewr_mean, rewr_mean_plus_rewr, orig_or_rewr_mean, orig_or_rewr_or_equal, \\\n",
    "                                        orig_or_rewr_plus_rewr_mean, rewriting_time] + \\\n",
    "                    list_original + [orig_med, orig_std] + list_rewritten + [rewr_med, rewr_std, rewr_med_plus_rewr]\n",
    "    #print(list_output)\n",
    "    file_path = \"results/DDB_Scala_comparison_TO_augment_server.csv\"\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a37073-1ff5-4bf4-becd-d37a900bef08",
   "metadata": {},
   "source": [
    "Create the output csv with the header. We add the running times for each query then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b905a-be9a-44ee-8c69-e7fb2c8114c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/DDB_Scala_comparison_TO_augment_server.csv\"\n",
    "\n",
    "names = [\"bench\", \"query\", \"orig mean\", \"rewr mean\", \"rewr mean+rewr\", \"orig/rewr(mean)\", \"orig/rewr/equal\", \"orig/rewr+rewr(mean)\", \"rewriting\", \n",
    "         \"orig 1\", \"orig 2\", \"orig 3\", \"orig 4\", \"orig 5\", \"orig med\", \"orig_std\", \"rewr 1\", \"rewr 2\", \"rewr 3\", \"rewr 4\", \"rewr 5\", \"rewr med\", \n",
    "         \"rewr_std\", \"rewr med+rewr\", ]\n",
    "\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce0077-b84a-4f03-abda-2045a18985dc",
   "metadata": {},
   "source": [
    "Connect to DuckDB for each dataset and execute all queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d8f12-ef70-40db-9938-3e88f516f1ff",
   "metadata": {},
   "source": [
    "### STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59654e-5a33-4a43-ab6c-06ce541562aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"stats\"\n",
    "conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT inet_server_addr(), inet_server_port()\")\n",
    "host, port = cur.fetchone()\n",
    "\n",
    "print(\"Host:\", host)\n",
    "print(\"Port:\", port)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5af08-5db1-4551-993d-bb0ce2d9a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=\"stats/stats.duckdb\")\n",
    "con.execute(\"INSTALL postgres\")\n",
    "con.execute(\"LOAD postgres\")\n",
    "con.execute(\"ATTACH 'host=\" + host + \" port=5432 user=postgres password=postgres dbname=stats' AS stats_DDB (TYPE postgres)\")\n",
    "con.execute(\"USE stats_DDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815eb70-b02e-45a0-b49a-5f4404ff17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('STATS')][0:1834]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb89deb-4df9-4e2a-87a7-fe13c04c8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][1834:1837]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88123e-3a96-4919-925f-dca6ec255af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][1837:1845]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe89853-cf01-47e1-b929-7f820f05492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][1845:1849]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211087d-30e1-4c1e-b7c4-5aa071b739d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][1849:1857]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b1a99-f099-4492-a846-c19c052efa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][1857:]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c61cf-476a-4966-a227-21a756650f85",
   "metadata": {},
   "source": [
    "### SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c122ac-406b-4dac-864d-49b5be901e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"snap\"\n",
    "conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT inet_server_addr(), inet_server_port()\")\n",
    "host, port = cur.fetchone()\n",
    "\n",
    "print(\"Host:\", host)\n",
    "print(\"Port:\", port)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e80bad-ce84-4ef6-8a5d-6198186f83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=\"snap/snap.duckdb\")\n",
    "con.execute(\"INSTALL postgres\")\n",
    "con.execute(\"LOAD postgres\")\n",
    "con.execute(\"ATTACH 'host=\" + host + \" port=5432 user=postgres password=postgres dbname=snap' AS snap_DDB (TYPE postgres)\")\n",
    "con.execute(\"USE snap_DDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0b916-8adc-4078-a751-9084a3066a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][0:7]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071411ab-5a3d-4961-9f73-e52338a75a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][7:61]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb558f58-e417-4af6-9323-da32e64f1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][61:64]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc2ad5-fd84-4f65-9e8f-dfe2af707903",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][64:122]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063e124-c3aa-4e70-b1f7-f00b16f57ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][122:129]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fefdf-40b7-4b56-a7b3-e3f53abc7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][129:]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc3cb4-07a3-4e77-a8f0-e4142c5b3a74",
   "metadata": {},
   "source": [
    "### JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cac82e-64c8-4f67-b7c3-e09c36cad71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"imdb\"\n",
    "conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT inet_server_addr(), inet_server_port()\")\n",
    "host, port = cur.fetchone()\n",
    "\n",
    "print(\"Host:\", host)\n",
    "print(\"Port:\", port)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a3f4a-492c-4f5e-a222-b4a7e01cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=\"job/job.duckdb\")\n",
    "con.execute(\"INSTALL postgres\")\n",
    "con.execute(\"LOAD postgres\")\n",
    "con.execute(\"ATTACH 'host=\" + host + \" port=5432 user=postgres password=postgres dbname=imdb' AS job_DDB (TYPE postgres)\")\n",
    "con.execute(\"USE job_DDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067b0d9-6c73-4590-a8b3-aaf883a883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('JOB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e89aa0-2e25-46d7-90e0-300f131e53ee",
   "metadata": {},
   "source": [
    "### LSQB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c9b19-4b92-40bd-8799-fac534884d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"lsqb\"\n",
    "conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT inet_server_addr(), inet_server_port()\")\n",
    "host, port = cur.fetchone()\n",
    "\n",
    "print(\"Host:\", host)\n",
    "print(\"Port:\", port)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5db85-9f84-4c9c-aec2-48c3f610ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=\"lsqb/lsqb.duckdb\")\n",
    "con.execute(\"INSTALL postgres\")\n",
    "con.execute(\"LOAD postgres\")\n",
    "con.execute(\"ATTACH 'host=\" + host + \" port=5432 user=postgres password=postgres dbname=lsqb' AS lsqb_DDB (TYPE postgres)\")\n",
    "con.execute(\"USE lsqb_DDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc16d52-b608-48ba-a9d0-11f71ae21c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('LSQB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665d6b1-e165-43a5-ad64-0c92fcb6e92b",
   "metadata": {},
   "source": [
    "### HETIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfe409-f740-4bcf-b051-34f20b674f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"hetio\"\n",
    "conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT inet_server_addr(), inet_server_port()\")\n",
    "host, port = cur.fetchone()\n",
    "\n",
    "print(\"Host:\", host)\n",
    "print(\"Port:\", port)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faa68e-cd8a-468b-b6bb-8a9da9cd7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=\"hetio/hetio.duckdb\")\n",
    "con.execute(\"INSTALL postgres\")\n",
    "con.execute(\"LOAD postgres\")\n",
    "con.execute(\"ATTACH 'host=\" + host + \" port=5432 user=postgres password=postgres dbname=hetio' AS hetio_DDB (TYPE postgres)\")\n",
    "con.execute(\"USE hetio_DDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06cc75-585e-4848-8826-e35a80c888cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('HETIO')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
