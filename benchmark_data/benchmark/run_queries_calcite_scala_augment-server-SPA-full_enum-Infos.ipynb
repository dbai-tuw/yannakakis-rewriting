{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e4175a-8e11-4ab2-b5f2-af33b04201ab",
   "metadata": {},
   "source": [
    "# Running queries using JSONS produced by Scala code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe4fae-04cc-4b9c-beb7-510662a4660d",
   "metadata": {},
   "source": [
    "Install and import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fa7d7-3b51-416f-b221-1369637b4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6645fd-f261-4197-ab65-bac11ae4cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import multiprocessing\n",
    "import signal\n",
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from py4j.protocol import Py4JJavaError, Py4JError\n",
    "import psutil\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c950d3-619f-4c9c-9eac-0853eb8242d4",
   "metadata": {},
   "source": [
    "Function for running one query. This means\n",
    "*  run the original query 5 times (after one initial run, which we do not use)\n",
    "*  run the rewritten queries 5 times (after one initial run, which we do not use) and drop the created tables each time\n",
    "*  take the runtimes and calculate mean, median and standard deviation of time for either the original or rewritten query\n",
    "*  additionaly save the runtimes for each stage\n",
    "*  save everything in a csv output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9533a-d16e-443e-ab3d-3d9e39229a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "def create_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"app\") \\\n",
    "        .master(f'local[{SPARK_CORES}]') \\\n",
    "        .config(\"spark.driver.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.executor.memory\", f'{SPARK_MEMORY}g') \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",False) \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.3.3.jar\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# import the database into Spark from PostgreSQL\n",
    "def import_db(spark, dbname):\n",
    "\n",
    "    if dbname == \"JOB\":\n",
    "        dbname = \"imdb\"\n",
    "    else:\n",
    "        dbname = dbname.lower()\n",
    "    \n",
    "    username = dbname\n",
    "    password = dbname\n",
    "    dbname = dbname\n",
    "\n",
    "    df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f'jdbc:postgresql://postgres:5432/{dbname}') \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "    for idx, row in df_tables.toPandas().iterrows():\n",
    "        if row.table_schema == 'public':\n",
    "            table_name = row.table_name\n",
    "            df = spark.read.format(\"jdbc\") \\\n",
    "                .option(\"url\", f'jdbc:postgresql://{DBHOST}:5432/{dbname}') \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"user\", username) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .load()\n",
    "    \n",
    "            print(table_name)\n",
    "            #print(df.show())\n",
    "            df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c63c9c-bc9d-45d1-85ab-5ea66178b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for handling TO and cancelling those queries in case of a TO\n",
    "def measure_resource_usage(resource_usage):\n",
    "    t = threading.current_thread()\n",
    "    secs = 0\n",
    "    while getattr(t, \"do_run\", True):\n",
    "        resource_usage.append(get_resource_usage(secs))\n",
    "        #print(\"resource usage: \" + str(resource_usage))\n",
    "        secs += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "def get_resource_usage(t):\n",
    "    return {\n",
    "        'time': t,\n",
    "        'memory': psutil.virtual_memory(),\n",
    "        'cpu': psutil.cpu_percent(interval=None, percpu=True),\n",
    "        'cpu_total': psutil.cpu_percent(interval=None, percpu=False)\n",
    "    }\n",
    "    \n",
    "def cancel_query(spark, seconds, group_id):\n",
    "    time.sleep(seconds)\n",
    "    print(\"cancelling jobs with id \" + group_id)\n",
    "    print(spark.sparkContext.cancelJobGroup(group_id))\n",
    "    print(\"cancelled job\")\n",
    "\n",
    "def cancel_query_after(spark, seconds):\n",
    "    group_id = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(16)) #random id\n",
    "    spark.sparkContext.setJobGroup(group_id, group_id)\n",
    "    threading.Thread(target=cancel_query, args=(spark, seconds, group_id,)).start()\n",
    "    return group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66996ed5-ee53-41fe-987a-62dc1630ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run the query 6 times checking for TO and calculate and saving all values\n",
    "def run_query(benchmark, query, spark):\n",
    "    print(benchmark, query)\n",
    "    file_path = f'output/{benchmark}_{query}_output.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # get the original and rewritten query\n",
    "    original_query = json_data[\"original_query\"]\n",
    "    rewritten_query_list = json_data[\"rewritten_query\"]\n",
    "    rewriting_time = json_data[\"time\"]\n",
    "\n",
    "    rewritten_query_list_stage1 = rewritten_query_list[:-2]\n",
    "    rewritten_query_list_stage3 = rewritten_query_list[-2:]\n",
    "    rewritten_query_list_stage2 = [r for r in rewritten_query_list_stage1 if \"stage2\" in r]\n",
    "    rewritten_query_list_stage0 = [r for r in rewritten_query_list_stage1 if \"VIEW\" in r]\n",
    "    rewritten_query_list_stage1 = [r for r in rewritten_query_list_stage1 if \"stage2\" not in r and \"VIEW\" not in r]\n",
    "\n",
    "    # change the queries such that they can be executed in SparkSQL (without changing the output)\n",
    "    rewritten_query_list_stage0 = [rewritten_query.replace(\" UNLOGGED TABLE \", \" VIEW \")\n",
    "                                                 .replace(\"TIMESTAMP(0)\", \"TIMESTAMP\")\n",
    "                                                 .replace(\"$\", \"_\")\n",
    "                                                 .replace(\"CREATE VIEW\", \"CREATE TEMPORARY VIEW\")\n",
    "                                  for rewritten_query in rewritten_query_list_stage0]\n",
    "    rewritten_query_list_stage1 = [rewritten_query.replace(\" UNLOGGED TABLE \", \" VIEW \")\n",
    "                                                 .replace(\"TIMESTAMP(0)\", \"TIMESTAMP\")\n",
    "                                                 .replace(\"$\", \"_\")\n",
    "                                                 .replace(\"CREATE VIEW\", \"CREATE TEMPORARY VIEW\")\n",
    "                                  for rewritten_query in rewritten_query_list_stage1]\n",
    "    rewritten_query_list_stage2 = [rewritten_query.replace(\" UNLOGGED TABLE \", \" VIEW \")\n",
    "                                                 .replace(\"TIMESTAMP(0)\", \"TIMESTAMP\")\n",
    "                                                 .replace(\"$\", \"_\")\n",
    "                                                 .replace(\"CREATE VIEW\", \"CREATE TEMPORARY VIEW\")\n",
    "                                  for rewritten_query in rewritten_query_list_stage2]\n",
    "    rewritten_query_list_stage3 = [rewritten_query.replace(\" UNLOGGED TABLE \", \" VIEW \")\n",
    "                                                 .replace(\"TIMESTAMP(0)\", \"TIMESTAMP\")\n",
    "                                                 .replace(\"$\", \"_\")\n",
    "                                                 .replace(\"CREATE VIEW\", \"CREATE TEMPORARY VIEW\")\n",
    "                                  for rewritten_query in rewritten_query_list_stage3]\n",
    "    rewritten_query_list = [rewritten_query.replace(\" UNLOGGED TABLE \", \" VIEW \")\n",
    "                                                 .replace(\"TIMESTAMP(0)\", \"TIMESTAMP\")\n",
    "                                                 .replace(\"$\", \"_\")\n",
    "                                                 .replace(\"CREATE VIEW\", \"CREATE TEMPORARY VIEW\")\n",
    "                                  for rewritten_query in rewritten_query_list]\n",
    "\n",
    "\n",
    "    # get the drop queries\n",
    "    file_path_drop = f'output/{benchmark}_{query}_drop.json'\n",
    "    with open(file_path_drop, 'r') as file:\n",
    "        json_drop = json.load(file)\n",
    "    drop_query_list = json_drop[\"rewritten_query\"]\n",
    "\n",
    "    drop_query_list = [drop_query.lower()\n",
    "                                       .replace(\"drop view\", \"drop view if exists\")\n",
    "                                       .replace(\"drop table\", \"drop table if exists\")\n",
    "                             for drop_query in drop_query_list]\n",
    "    drop_query_list[0] = drop_query_list[0].replace(\"table\", \"view\")\n",
    "\n",
    "    for drop_query in drop_query_list:\n",
    "            drop = spark.sql(drop_query)\n",
    "            drop.show()\n",
    "\n",
    "    timeout_flag_orig = True\n",
    "    timeout_flag_rewr = True\n",
    "\n",
    "    # the first run is just a warm up run and to check for the time out\n",
    "    try:\n",
    "        spark.sparkContext._jvm.System.gc()\n",
    "\n",
    "        start_time = time.time()\n",
    "        resource_usage = []\n",
    "        measure_thread = threading.Thread(target=measure_resource_usage, args=(resource_usage, ))\n",
    "        measure_thread.start()\n",
    "    \n",
    "        group_id = cancel_query_after(spark, TIMEOUT)\n",
    "        result = spark.sql(original_query)\n",
    "        #result.show()\n",
    "        rows_orig = result.count()\n",
    "        end_time = time.time()\n",
    "        print(end_time-start_time)\n",
    "    \n",
    "        measure_thread.do_run = False\n",
    "        timeout_flag_orig = False\n",
    "    except Py4JError as e:\n",
    "        print('timeout or error orig: ' + str(e))\n",
    "        \n",
    "    try:\n",
    "        spark.sparkContext._jvm.System.gc()\n",
    "        resource_usage = []\n",
    "        measure_thread = threading.Thread(target=measure_resource_usage, args=(resource_usage, ))\n",
    "        measure_thread.start()\n",
    "\n",
    "        group_id = cancel_query_after(spark, TIMEOUT)\n",
    "        for rewritten_query in rewritten_query_list:\n",
    "            if rewritten_query.startswith(\"SELECT\"):\n",
    "                result1 = spark.sql(rewritten_query)\n",
    "                #result1.show()\n",
    "                rows_rewr = result1.count()\n",
    "            else:\n",
    "                result2 = spark.sql(rewritten_query)\n",
    "                result2.show()\n",
    "\n",
    "        measure_thread.do_run = False\n",
    "        timeout_flag_rewr = False\n",
    "\n",
    "        for drop_query in drop_query_list:\n",
    "            drop = spark.sql(drop_query)\n",
    "            drop.show()\n",
    "        \n",
    "    except Py4JError as e:\n",
    "        print('timeout or error rewr: ' + str(e))\n",
    "\n",
    "    print(timeout_flag_orig, timeout_flag_rewr)\n",
    "    # original and rewritten query are TOs\n",
    "    if timeout_flag_orig and timeout_flag_rewr:\n",
    "        list_original_time = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "\n",
    "        list_rewritten_time_stage0 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage0_mean = \"-\"\n",
    "        rewr_stage0_med = \"-\"\n",
    "        rewr_stage0_std = \"-\"\n",
    "        list_rewritten_time_stage1 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage1_mean = \"-\"\n",
    "        rewr_stage1_med = \"-\"\n",
    "        rewr_stage1_std = \"-\"\n",
    "        list_rewritten_time_stage2 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage2_mean = \"-\"\n",
    "        rewr_stage2_med = \"-\"\n",
    "        rewr_stage2_std = \"-\"\n",
    "        list_rewritten_time_stage3 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage3_mean = \"-\"\n",
    "        rewr_stage3_med = \"-\"\n",
    "        rewr_stage3_std = \"-\"\n",
    "        list_rewritten_time = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "\n",
    "        orig_rewr = \"-\"\n",
    "        rows = \"-\"\n",
    "        \n",
    "    # original query is a TO and the rewritten not\n",
    "    elif timeout_flag_orig:\n",
    "        list_original_time = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "\n",
    "        list_rewritten_time_stage0 = []\n",
    "        list_rewritten_time_stage1 = []\n",
    "        list_rewritten_time_stage2 = []\n",
    "        list_rewritten_time_stage3 = []\n",
    "        list_rewritten_time = []\n",
    "        \n",
    "        print(\"rewritten\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "\n",
    "            try:\n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage0 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage0:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage0 = time.time()\n",
    "                rewritten_time_stage0 = end_time_rewritten_stage0 - start_time_rewritten_stage0\n",
    "    \n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage1 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage1:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage1 = time.time()\n",
    "                rewritten_time_stage1 = end_time_rewritten_stage1 - start_time_rewritten_stage1\n",
    "    \n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage2 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage2:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage2 = time.time()\n",
    "                rewritten_time_stage2 = end_time_rewritten_stage2 - start_time_rewritten_stage2\n",
    "    \n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage3 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage3:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage3 = time.time()\n",
    "                rewritten_time_stage3 = end_time_rewritten_stage3 - start_time_rewritten_stage3\n",
    "\n",
    "                list_rewritten_time_stage0.append(rewritten_time_stage0)\n",
    "                list_rewritten_time_stage1.append(rewritten_time_stage1)\n",
    "                list_rewritten_time_stage2.append(rewritten_time_stage2)\n",
    "                list_rewritten_time_stage3.append(rewritten_time_stage3)\n",
    "                list_rewritten_time.append(rewritten_time_stage0 + rewritten_time_stage1 + rewritten_time_stage2 + rewritten_time_stage3) \n",
    "\n",
    "            except Exception as e:\n",
    "                list_rewritten_time_stage0.append(\"-\")\n",
    "                list_rewritten_time_stage1.append(\"-\")\n",
    "                list_rewritten_time_stage2.append(\"-\")\n",
    "                list_rewritten_time_stage3.append(\"-\")\n",
    "                list_rewritten_time.append(\"-\") \n",
    "                \n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list:\n",
    "                drop = spark.sql(drop_query)\n",
    "                drop.show()\n",
    "\n",
    "        list_rewritten_time_stage0_filtered = [x for x in list_rewritten_time_stage0 if x != \"-\"]\n",
    "        list_rewritten_time_stage1_filtered = [x for x in list_rewritten_time_stage1 if x != \"-\"]\n",
    "        list_rewritten_time_stage2_filtered = [x for x in list_rewritten_time_stage2 if x != \"-\"]\n",
    "        list_rewritten_time_stage3_filtered = [x for x in list_rewritten_time_stage3 if x != \"-\"]\n",
    "        list_rewritten_time_filtered = [x for x in list_rewritten_time if x != \"-\"]\n",
    "        rewr_stage0_mean = np.mean(list_rewritten_time_stage0_filtered)\n",
    "        rewr_stage0_med = np.median(list_rewritten_time_stage0_filtered)\n",
    "        rewr_stage0_std = np.std(list_rewritten_time_stage0_filtered)\n",
    "        rewr_stage1_mean = np.mean(list_rewritten_time_stage1_filtered)\n",
    "        rewr_stage1_med = np.median(list_rewritten_time_stage1_filtered)\n",
    "        rewr_stage1_std = np.std(list_rewritten_time_stage1_filtered)\n",
    "        rewr_stage2_mean = np.mean(list_rewritten_time_stage2_filtered)\n",
    "        rewr_stage2_med = np.median(list_rewritten_time_stage2_filtered)\n",
    "        rewr_stage2_std = np.std(list_rewritten_time_stage2_filtered)\n",
    "        rewr_stage3_mean = np.mean(list_rewritten_time_stage3_filtered)\n",
    "        rewr_stage3_med = np.median(list_rewritten_time_stage3_filtered)\n",
    "        rewr_stage3_std = np.std(list_rewritten_time_stage3_filtered)\n",
    "        rewr_mean = np.mean(list_rewritten_time_filtered)\n",
    "        rewr_med = np.median(list_rewritten_time_filtered)\n",
    "        rewr_std = np.std(list_rewritten_time_filtered)\n",
    "        \n",
    "        orig_rewr = \"rewr\"\n",
    "        rows = rows_rewr\n",
    "\n",
    "    # rewritten query is a TO and the original not\n",
    "    elif timeout_flag_rewr:\n",
    "        list_original_time = []\n",
    "        print(\"orig\")\n",
    "        for i in range(5):\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "\n",
    "            try:\n",
    "                # execute the original query\n",
    "                start_time_original = time.time()\n",
    "                exec = spark.sql(original_query)\n",
    "                exec.show()\n",
    "                end_time_original = time.time()\n",
    "                original_time = end_time_original - start_time_original\n",
    "                list_original_time.append(original_time)\n",
    "            except Exception as e:\n",
    "                list_original_time.append(\"-\")\n",
    "\n",
    "        list_original_time_filtered = [x for x in list_original_time if x != \"-\"]\n",
    "        orig_mean = np.mean(list_original_time_filtered)\n",
    "        orig_med = np.median(list_original_time_filtered)\n",
    "        orig_std = np.std(list_original_time_filtered)\n",
    "\n",
    "        list_rewritten_time_stage0 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage0_mean = \"-\"\n",
    "        rewr_stage0_med = \"-\"\n",
    "        rewr_stage0_std = \"-\"\n",
    "        list_rewritten_time_stage1 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage1_mean = \"-\"\n",
    "        rewr_stage1_med = \"-\"\n",
    "        rewr_stage1_std = \"-\"\n",
    "        list_rewritten_time_stage2 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage2_mean = \"-\"\n",
    "        rewr_stage2_med = \"-\"\n",
    "        rewr_stage2_std = \"-\"\n",
    "        list_rewritten_time_stage3 = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_stage3_mean = \"-\"\n",
    "        rewr_stage3_med = \"-\"\n",
    "        rewr_stage3_std = \"-\"\n",
    "        list_rewritten_time = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        \n",
    "        orig_rewr = \"orig\"\n",
    "        rows = rows_orig\n",
    "\n",
    "    # both queries are no TOs\n",
    "    else:\n",
    "        #print(result, result1)\n",
    "        list_original_time = []\n",
    "        list_rewritten_time_stage0 = []\n",
    "        list_rewritten_time_stage1 = []\n",
    "        list_rewritten_time_stage2 = []\n",
    "        list_rewritten_time_stage3 = []\n",
    "        list_rewritten_time = []\n",
    "        \n",
    "        # take times for 5 runs (run 2-6) for the original query and the rewritten query\n",
    "        print(\"orig+rewr\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "            \n",
    "            try:\n",
    "                # execute the original query\n",
    "                start_time_original = time.time()\n",
    "                exec = spark.sql(original_query)\n",
    "                exec.show()\n",
    "                end_time_original = time.time()\n",
    "                original_time = end_time_original - start_time_original\n",
    "                list_original_time.append(original_time)\n",
    "            except Exception as e:\n",
    "                list_original_time.append(\"-\")\n",
    "\n",
    "            spark.sparkContext._jvm.System.gc()\n",
    "\n",
    "            try:\n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage0 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage0:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage0 = time.time()\n",
    "                rewritten_time_stage0 = end_time_rewritten_stage0 - start_time_rewritten_stage0\n",
    "    \n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage1 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage1:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage1 = time.time()\n",
    "                rewritten_time_stage1 = end_time_rewritten_stage1 - start_time_rewritten_stage1\n",
    "    \n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage2 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage2:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage2 = time.time()\n",
    "                rewritten_time_stage2 = end_time_rewritten_stage2 - start_time_rewritten_stage2\n",
    "    \n",
    "                # execute the rewritten query\n",
    "                start_time_rewritten_stage3 = time.time()\n",
    "                for rewritten_query in rewritten_query_list_stage3:\n",
    "                    exec = spark.sql(rewritten_query)\n",
    "                    exec.show()\n",
    "                end_time_rewritten_stage3 = time.time()\n",
    "                rewritten_time_stage3 = end_time_rewritten_stage3 - start_time_rewritten_stage3\n",
    "\n",
    "                list_rewritten_time_stage0.append(rewritten_time_stage0)\n",
    "                list_rewritten_time_stage1.append(rewritten_time_stage1)\n",
    "                list_rewritten_time_stage2.append(rewritten_time_stage2)\n",
    "                list_rewritten_time_stage3.append(rewritten_time_stage3)\n",
    "                list_rewritten_time.append(rewritten_time_stage0 + rewritten_time_stage1 + rewritten_time_stage2 + rewritten_time_stage3) \n",
    "\n",
    "            except Exception as e:\n",
    "                list_rewritten_time_stage0.append(\"-\")\n",
    "                list_rewritten_time_stage1.append(\"-\")\n",
    "                list_rewritten_time_stage2.append(\"-\")\n",
    "                list_rewritten_time_stage3.append(\"-\")\n",
    "                list_rewritten_time.append(\"-\") \n",
    "                \n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list:\n",
    "                drop = spark.sql(drop_query)\n",
    "                drop.show()\n",
    "\n",
    "        list_original_time_filtered = [x for x in list_original_time if x != \"-\"]\n",
    "        orig_mean = np.mean(list_original_time_filtered)\n",
    "        orig_med = np.median(list_original_time_filtered)\n",
    "        orig_std = np.std(list_original_time_filtered)\n",
    "        list_rewritten_time_stage0_filtered = [x for x in list_rewritten_time_stage0 if x != \"-\"]\n",
    "        list_rewritten_time_stage1_filtered = [x for x in list_rewritten_time_stage1 if x != \"-\"]\n",
    "        list_rewritten_time_stage2_filtered = [x for x in list_rewritten_time_stage2 if x != \"-\"]\n",
    "        list_rewritten_time_stage3_filtered = [x for x in list_rewritten_time_stage3 if x != \"-\"]\n",
    "        list_rewritten_time_filtered = [x for x in list_rewritten_time if x != \"-\"]\n",
    "        rewr_stage0_mean = np.mean(list_rewritten_time_stage0_filtered)\n",
    "        rewr_stage0_med = np.median(list_rewritten_time_stage0_filtered)\n",
    "        rewr_stage0_std = np.std(list_rewritten_time_stage0_filtered)\n",
    "        rewr_stage1_mean = np.mean(list_rewritten_time_stage1_filtered)\n",
    "        rewr_stage1_med = np.median(list_rewritten_time_stage1_filtered)\n",
    "        rewr_stage1_std = np.std(list_rewritten_time_stage1_filtered)\n",
    "        rewr_stage2_mean = np.mean(list_rewritten_time_stage2_filtered)\n",
    "        rewr_stage2_med = np.median(list_rewritten_time_stage2_filtered)\n",
    "        rewr_stage2_std = np.std(list_rewritten_time_stage2_filtered)\n",
    "        rewr_stage3_mean = np.mean(list_rewritten_time_stage3_filtered)\n",
    "        rewr_stage3_med = np.median(list_rewritten_time_stage3_filtered)\n",
    "        rewr_stage3_std = np.std(list_rewritten_time_stage3_filtered)\n",
    "        rewr_mean = np.mean(list_rewritten_time_filtered)\n",
    "        rewr_med = np.median(list_rewritten_time_filtered)\n",
    "        rewr_std = np.std(list_rewritten_time_filtered)\n",
    "            \n",
    "        if orig_med > rewr_med:\n",
    "            orig_rewr = \"rewr\"\n",
    "        else:\n",
    "            orig_rewr = \"orig\"\n",
    "    \n",
    "        if rows_orig == rows_rewr:\n",
    "            rows = rows_orig\n",
    "        else:\n",
    "            rows = \"not the same!\"\n",
    "\n",
    "    if benchmark == \"IMDB\":\n",
    "        benchmark = \"JOB\"\n",
    "    list_output = [benchmark, query, orig_rewr, orig_med, rewr_med, rewr_stage0_med, rewr_stage1_med, rewr_stage2_med, rewr_stage3_med, rows] + \\\n",
    "                    list_original_time + [orig_mean, orig_std] + list_rewritten_time + [rewr_mean, rewr_std] + list_rewritten_time_stage0 + \\\n",
    "                    [rewr_stage0_mean, rewr_stage0_std] + list_rewritten_time_stage1 + [rewr_stage1_mean, rewr_stage1_std] + \\\n",
    "                    list_rewritten_time_stage2 + [rewr_stage2_mean, rewr_stage2_std] + list_rewritten_time_stage3 + [rewr_stage3_mean, rewr_stage3_std]\n",
    "\n",
    "    #print(list_output)\n",
    "    file_path = \"results/SPA_Scala_comparison_TO_augment_server_full_enum_infos.csv\"\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000f324-b53b-486e-b554-643d5fb9cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "SPARK_MEMORY = 120\n",
    "SPARK_CORES = 4\n",
    "DBHOST = 'postgres'\n",
    "TIMEOUT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b709d2-6388-4456-991e-a987d9d7bf6b",
   "metadata": {},
   "source": [
    "Create the output csv with the header. We add the running times for each query then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15345ee0-b848-4421-bfe2-8e74b3aa775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/SPA_Scala_comparison_TO_augment_server_full_enum_infos.csv\"\n",
    "\n",
    "names = [\"bench\", \"query\", \"orig/rewr(med)\", \"orig(med)\", \"rewr(med)\", \"stage0(med)\", \"stage1(med)\", \"stage2(med)\", \"stage3(med)\", \"rows\",\n",
    "        \"orig 1\", \"orig 2\", \"orig 3\", \"orig 4\", \"orig 5\", \"orig(mean)\", \"orig(std)\", \n",
    "        \"rewr 1\", \"rewr 2\", \"rewr 3\", \"rewr 4\", \"rewr 5\", \"rewr(mean)\", \"rewr(std)\",\n",
    "        \"stage0 1\", \"stage0 2\", \"stage0 3\", \"stage0 4\", \"stage0 5\", \"stage0(mean)\", \"stage0(std)\",\n",
    "        \"stage1 1\", \"stage1 2\", \"stage1 3\", \"stage1 4\", \"stage1 5\", \"stage1(mean)\", \"stage1(std)\",\n",
    "        \"stage2 1\", \"stage2 2\", \"stage2 3\", \"stage2 4\", \"stage2 5\", \"stage2(mean)\", \"stage2(std)\",\n",
    "        \"stage3 1\", \"stage3 2\", \"stage3 3\", \"stage3 4\", \"stage3 5\", \"stage3(mean)\", \"stage3(std)\"]\n",
    "\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434e62b-fca5-4eb4-a260-ee5c5591f617",
   "metadata": {},
   "source": [
    "Connect to Spark for each dataset and execute all queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896e8fd-1dcf-44e7-abb6-0fc8ee9f61ad",
   "metadata": {},
   "source": [
    "## STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70cbfb-802d-403b-94c8-9bf9e634e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"STATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c56465-ff3a-4cd9-8512-38189c37d0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'output/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('STATS')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c61cf-476a-4966-a227-21a756650f85",
   "metadata": {},
   "source": [
    "### SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8768f-5419-44b9-87fd-69beb2004d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"SNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0b916-8adc-4078-a751-9084a3066a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'output/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc3cb4-07a3-4e77-a8f0-e4142c5b3a74",
   "metadata": {},
   "source": [
    "### JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7789b70-2122-4726-9492-1a7b54cbf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"JOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067b0d9-6c73-4590-a8b3-aaf883a883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'output/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('IMDB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e89aa0-2e25-46d7-90e0-300f131e53ee",
   "metadata": {},
   "source": [
    "### LSQB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4351a-523f-47b9-ad4e-b978a33e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"LSQB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc16d52-b608-48ba-a9d0-11f71ae21c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'output/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('LSQB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665d6b1-e165-43a5-ad64-0c92fcb6e92b",
   "metadata": {},
   "source": [
    "### HETIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d09b4-abf3-43c8-b24d-24af457247ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()\n",
    "import_db(spark, \"HETIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bb457-cbe1-4a40-9935-7248199ddd31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'output/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('HETIO')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1], spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
