{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cbf1d8-de7f-4b5b-b13c-4f92a846298f",
   "metadata": {},
   "source": [
    "# Get all features together with the evaluation times for all queries (DuckDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaa467-7734-447b-bf41-1238d0f68c63",
   "metadata": {},
   "source": [
    "Features based on the query:   \n",
    "*  number of relations\n",
    "*  number of conditions\n",
    "*  number of filters\n",
    "*  number of joins\n",
    "\n",
    "Features based on the join tree:\n",
    "*  depth\n",
    "*  container count (min, max, mean, median, q1, q3)\n",
    "*  branching factors (min, max, mean, median, q1, q3)\n",
    "*  balancedness factor \n",
    "\n",
    "Features based on the data in the database (EXPLAIN):\n",
    "*  estimated cardinalities (of tables, filters, joins) (min, max, mean, median, q1, q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4fd341-9bd9-4d8e-b22f-f27949dc1c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Downloading duckdb-0.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 97.5 MB/s eta 0:00:00\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-0.10.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (2.9.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install pandas\n",
    "pip install duckdb\n",
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb971097-ee96-48f8-ae56-ef8d2ceb2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db751e9-7b01-48d3-b378-beb7838bb418",
   "metadata": {},
   "source": [
    "### Get the features based on the structure of the query and database information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c308525-8e8f-4ab6-a3c6-ef7d8f31172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_through_plan(plan, table_rows, join_rows):\n",
    "    if plan[\"Node Type\"] == \"Seq Scan\":\n",
    "        table_rows.append(plan[\"Plan Rows\"])\n",
    "    elif plan[\"Node Type\"] == \"Index Only Scan\":\n",
    "        table_rows.append(plan[\"Plan Rows\"])\n",
    "    elif plan[\"Node Type\"] == \"Index Scan\":\n",
    "        table_rows.append(plan[\"Plan Rows\"])\n",
    "    elif plan[\"Node Type\"] == \"Bitmap Index Scan\":\n",
    "        table_rows.append(plan[\"Plan Rows\"])\n",
    "    \n",
    "    if plan[\"Node Type\"] == \"Hash Join\":\n",
    "        join_rows.append(plan[\"Plan Rows\"])\n",
    "    elif plan[\"Node Type\"] == \"Merge Join\":\n",
    "        join_rows.append(plan[\"Plan Rows\"])\n",
    "    elif plan[\"Node Type\"] == \"Nested Loop\":\n",
    "        join_rows.append(plan[\"Plan Rows\"])\n",
    "\n",
    "    if \"Plans\" in plan.keys():\n",
    "        for i in range(len(plan[\"Plans\"])):\n",
    "            iterate_through_plan(plan[\"Plans\"][i], table_rows, join_rows)\n",
    "\n",
    "    return table_rows, join_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e4e0be-b09f-40ac-be0b-f83d7603b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c27de6-5657-4087-b9fc-6e071f7343a6",
   "metadata": {},
   "source": [
    "1. features only of the query structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d406e1e-372a-45c5-b210-b36823b051e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "input_file = 'scala_commands_augment_filter_agg.txt'\n",
    "output_file = 'results/featuresDatabase_DDB.csv'\n",
    "\n",
    "# Open input and output files\n",
    "with open(input_file, 'r') as f_input, open(output_file, 'w', newline='') as f_output:\n",
    "    csv_writer = csv.writer(f_output)\n",
    "    \n",
    "    # Write header to CSV file\n",
    "    csv_writer.writerow(['bench', 'query', '#relations', '#conditions', '#filters', '#joins', 'text'])\n",
    "\n",
    "    database_old = \" \"\n",
    "    # Read input file line by line\n",
    "    for line in f_input:\n",
    "        # Split each line into components\n",
    "        pattern = r'(?<!\\\\)\\\"|\\\"(?<!\\\\)(?=\\s+\\\"|$)'\n",
    "        components = re.split(pattern, line)\n",
    "        \n",
    "        # Extract relevant information\n",
    "        benchmark = components[3]\n",
    "        number = components[5]\n",
    "        query = components[1].strip()\n",
    "\n",
    "        # FEATURES BASED ON QUERY STRUCTURE\n",
    "        # get the number of relations\n",
    "        query_upper = query.upper()\n",
    "        from_index = query_upper.find(\"FROM\")\n",
    "        where_index = query_upper.find(\"WHERE\")\n",
    "        number_of_relations = query[from_index:where_index].count(\",\") + 1\n",
    "\n",
    "        # get the number of conditions\n",
    "        number_of_conditions = query.count(\"AND\") + 1\n",
    "\n",
    "        # get how many filter and join conditions\n",
    "        parts = query_upper.split(\"WHERE\")[1].split(\"AND\")\n",
    "        filter = 0\n",
    "        join = 0\n",
    "        joins = []\n",
    "        for p in parts:\n",
    "            partners = []\n",
    "            p_split = p.split(\"=\")\n",
    "            if len(p_split) == 2 and p_split[1].count(\"'\") == 0 and p_split[1].count('\"') == 0 and is_number(p_split[1].strip()) == False:\n",
    "                partners = [p2.strip().split(\".\")[0] for p2 in p.split(\"=\")]\n",
    "                if partners not in joins and list(reversed(partners)) not in joins:\n",
    "                    joins.append(partners)\n",
    "                    join += 1\n",
    "            else:\n",
    "                filter += 1\n",
    "\n",
    "        # Write data to CSV file\n",
    "        csv_writer.writerow([benchmark, number, number_of_relations, number_of_conditions, filter, join, query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda0ef9-bb55-4a05-8bde-92ed877cfffd",
   "metadata": {},
   "source": [
    "2. features of the query structure and of DuckDB EXPLAIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a11116-dd33-4e4e-b38a-6bdbb75bab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "input_file = 'scala_commands_augment_filter_agg.txt'\n",
    "output_file = 'results/featuresDatabase_DDB_extra.csv'\n",
    "\n",
    "# Open input and output files\n",
    "with open(input_file, 'r') as f_input, open(output_file, 'w', newline='') as f_output:\n",
    "    csv_writer = csv.writer(f_output)\n",
    "    \n",
    "    # Write header to CSV file\n",
    "    csv_writer.writerow(['bench', 'query', '#relations', '#conditions', '#filters', '#joins', 'min(est. cardinality)', 'max(est. cardinality)', \n",
    "                         'mean(est. cardinality)', 'q25(est. cardinality)', 'median(est. cardinality)', 'q75(est. cardinality)',\n",
    "                         'list est. cardinality', 'text'])\n",
    "\n",
    "    database_old = \" \"\n",
    "    # Read input file line by line\n",
    "    for line in f_input:\n",
    "        # Split each line into components\n",
    "        pattern = r'(?<!\\\\)\\\"|\\\"(?<!\\\\)(?=\\s+\\\"|$)'\n",
    "        components = re.split(pattern, line)\n",
    "        \n",
    "        # Extract relevant information\n",
    "        benchmark = components[3]\n",
    "        number = components[5]\n",
    "        query = components[1].strip()\n",
    "\n",
    "        # FEATURES BASED ON QUERY STRUCTURE\n",
    "        # get the number of relations\n",
    "        query_upper = query.upper()\n",
    "        from_index = query_upper.find(\"FROM\")\n",
    "        where_index = query_upper.find(\"WHERE\")\n",
    "        number_of_relations = query[from_index:where_index].count(\",\") + 1\n",
    "\n",
    "        # get the number of conditions\n",
    "        number_of_conditions = query.count(\"AND\") + 1\n",
    "\n",
    "        # get how many filter and join conditions\n",
    "        parts = query_upper.split(\"WHERE\")[1].split(\"AND\")\n",
    "        filter = 0\n",
    "        join = 0\n",
    "        joins = []\n",
    "        for p in parts:\n",
    "            partners = []\n",
    "            p_split = p.split(\"=\")\n",
    "            if len(p_split) == 2 and p_split[1].count(\"'\") == 0 and p_split[1].count('\"') == 0 and is_number(p_split[1].strip()) == False:\n",
    "                partners = [p2.strip().split(\".\")[0] for p2 in p.split(\"=\")]\n",
    "                if partners not in joins and list(reversed(partners)) not in joins:\n",
    "                    joins.append(partners)\n",
    "                    join += 1\n",
    "            else:\n",
    "                filter += 1\n",
    "\n",
    "        # FEATURES BASED ON DATABASE\n",
    "        # get features based on the DuckDB Plan by EXPLAIN\n",
    "        database = benchmark.lower() + \"/\" + benchmark.lower() + \".duckdb\"\n",
    "        if database != database_old:\n",
    "            if benchmark == \"JOB\":\n",
    "                database_pos = \"imdb\"\n",
    "            else:\n",
    "                database_pos = benchmark.lower()\n",
    "            conn = psycopg2.connect(\n",
    "                    host=\"postgres\",\n",
    "                    database=database_pos,\n",
    "                    user=database_pos,\n",
    "                    password=database_pos)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT inet_server_addr(), inet_server_port()\")\n",
    "            host, port = cur.fetchone()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            con = duckdb.connect(database=database)\n",
    "            con.execute(\"INSTALL postgres\")\n",
    "            con.execute(\"LOAD postgres\")\n",
    "            con.execute(\"ATTACH 'host=\" + host + \" port=5432 user=postgres password=postgres dbname=\" + database_pos + \"' AS \" + \n",
    "                        benchmark.lower() + \"_DDB (TYPE postgres)\")\n",
    "            con.execute(\"USE \" + benchmark.lower() + \"_DDB\")\n",
    "            database_old = database\n",
    "\n",
    "        conn = duckdb.connect(database=database)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE \" + benchmark.lower() + \"_DDB\")\n",
    "        query2 = query.replace('\\\\\\\\\\\\\"', '')\n",
    "        cur.execute(\"EXPLAIN \" + query2)\n",
    "        result = cur.fetchall()\n",
    "\n",
    "        pattern = r'EC:\\s*(\\w+)'\n",
    "        est_card = re.findall(pattern, result[0][1])\n",
    "\n",
    "        est_card = list(map(int, est_card))\n",
    "        # calculate min, max, mean, median, 0.25-quantile and 0.75-quantile\n",
    "        est_card_min = np.min(est_card)\n",
    "        est_card_max = np.max(est_card)\n",
    "        est_card_median = np.median(est_card)\n",
    "        est_card_mean = np.mean(est_card)\n",
    "        est_card_q25 = np.quantile(est_card, 0.25)\n",
    "        est_card_q75 = np.quantile(est_card, 0.75)\n",
    "                \n",
    "        # Write data to CSV file\n",
    "        csv_writer.writerow([benchmark, number, number_of_relations, number_of_conditions, filter, join, est_card_min, est_card_max, \n",
    "                             est_card_mean, est_card_q25, est_card_median, est_card_q75, est_card, query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42371ab7-d2dc-4a0c-b995-92a170de5af8",
   "metadata": {},
   "source": [
    "### Get the features based on the join tree structure (calculated in Scala, imported and formated here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd828bb-ad68-49c4-b1a6-b5f2229b09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'results/featuresScala_DDB.csv'\n",
    "csv_header = [\"bench\", \"query\", \"depth\", \"min(container counts)\", \"max(container counts)\", \"mean(container counts)\", \"q25(container counts)\",\n",
    "              \"median(container counts)\", \"q75(container counts)\", \"min(branching factors)\", \"max(branching factors)\", \"mean(branching factors)\", \n",
    "              \"median(branching factors)\", \"q25(branching factors)\", \"q75(branching factors)\", \"balancedness factor\", \"container counts list\", \n",
    "              \"branching factors list\"]\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(csv_header)\n",
    "\n",
    "    directory = 'rewritten/'\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\"output.json\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            benchmark = filename.split(\"_\")[0]\n",
    "            number = filename.split(\"_\")[1]\n",
    "            with open(filepath, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                # FEATURES BASED ON THE JOIN TREE STRUCTURE\n",
    "                feature = data.get(\"features\", [])\n",
    "                features = feature.split(\"List(\")\n",
    "                depth = int(features[1][:-2])\n",
    "                container_counts = [int(x) for x in features[2][:-3].split(\", \")]\n",
    "                container_counts_min = np.min(container_counts)\n",
    "                container_counts_max = np.max(container_counts)\n",
    "                container_counts_mean = np.mean(container_counts)\n",
    "                container_counts_median = np.median(container_counts)\n",
    "                container_counts_q25 = np.quantile(container_counts, 0.25)\n",
    "                container_counts_q75 = np.quantile(container_counts, 0.25)\n",
    "                branching_factors = [int(x) for x in features[3].split(\"), \")[0].split(\", \")]\n",
    "                branching_factors_min = np.min(branching_factors)\n",
    "                branching_factors_max = np.max(branching_factors)\n",
    "                branching_factors_mean = np.mean(branching_factors)\n",
    "                branching_factors_median = np.median(branching_factors)\n",
    "                branching_factors_q25 = np.quantile(branching_factors, 0.25)\n",
    "                branching_factors_q75 = np.quantile(branching_factors, 0.75)\n",
    "                balancedness_factor = float(features[3].split(\"), \")[1][:-1])\n",
    "\n",
    "                # Write data to CSV file\n",
    "                csv_row = [benchmark, number, depth, container_counts_min, container_counts_max, container_counts_mean, container_counts_q25,\n",
    "                           container_counts_median, container_counts_q75, branching_factors_min, branching_factors_max, branching_factors_mean,\n",
    "                           branching_factors_median, branching_factors_q25, branching_factors_q75, balancedness_factor, container_counts, \n",
    "                           branching_factors]\n",
    "                writer.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150916d8-d9b5-44dc-aa0a-2b9f5a820036",
   "metadata": {},
   "source": [
    "### Merge all features and evaluation times for each query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a7e1c-2ee2-4b9f-b015-b7479938ca19",
   "metadata": {},
   "source": [
    "1. Merge features of the query structure and the join tree with the evaluation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2afbc001-ca32-4855-9b72-319e361f6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/featuresDatabase_DDB.csv')\n",
    "df2 = pd.read_csv('results/featuresScala_DDB.csv')\n",
    "df3 = pd.read_csv('results/DDB_Scala_comparison_TO_augment_server.csv')\n",
    "df3['orig mean'] = df3['orig mean'].replace('TO', 100).astype(\"float64\")\n",
    "df3['rewr mean+rewr'] = df3['rewr mean+rewr'].replace('TO', 100).astype(\"float64\")\n",
    "df3['rewr mean'] = df3['rewr mean'].replace('TO', 100).astype(\"float64\")\n",
    "df3['diff rewr+rewr-orig'] = df3['rewr mean+rewr'] - df3['orig mean']\n",
    "df3['diff rewr-orig'] = df3['rewr mean'] - df3['orig mean']\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on=[\"bench\", \"query\"], how='inner').merge(df3, on=[\"bench\", \"query\"], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72269b-132d-4144-997d-3e4be152da69",
   "metadata": {},
   "source": [
    "2. Merge features of the query structure, the join tree and the DuckDB EXPLAIN with the evaluation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9ed7c9-c7de-47ad-9cc4-7e5c76ec25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_extra = pd.read_csv('results/featuresDatabase_DDB_extra.csv')\n",
    "df2_extra = pd.read_csv('results/featuresScala_DDB.csv')\n",
    "df3_extra = pd.read_csv('results/DDB_Scala_comparison_TO_augment_server.csv')\n",
    "df3_extra['orig mean'] = df3_extra['orig mean'].replace('TO', 100).astype(\"float64\")\n",
    "df3_extra['rewr mean+rewr'] = df3_extra['rewr mean+rewr'].replace('TO', 100).astype(\"float64\")\n",
    "df3_extra['rewr mean'] = df3_extra['rewr mean'].replace('TO', 100).astype(\"float64\")\n",
    "df3_extra['diff rewr+rewr-orig'] = df3_extra['rewr mean+rewr'] - df3_extra['orig mean']\n",
    "df3_extra['diff rewr-orig'] = df3_extra['rewr mean'] - df3_extra['orig mean']\n",
    "\n",
    "merged_df_extra = pd.merge(df1_extra, df2_extra, on=[\"bench\", \"query\"], how='inner').merge(df3_extra, on=[\"bench\", \"query\"], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b65b7-e81b-4e07-b676-054f4767a153",
   "metadata": {},
   "source": [
    "### Save the resulting dataframes as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d5607f-16bc-4238-a728-3c15fbc9973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[[\"bench\", \"query\", \"orig/rewr(mean)\", \"orig/rewr+rewr(mean)\", \"orig mean\", \"rewr mean\", \"rewr mean+rewr\", \"diff rewr-orig\", \n",
    "           \"diff rewr+rewr-orig\", \"#relations\", \"#conditions\", \"#filters\", \"#joins\", \"depth\", \"min(container counts)\", \"max(container counts)\", \n",
    "           \"mean(container counts)\", \"q25(container counts)\", \"median(container counts)\", \"q75(container counts)\", \"min(branching factors)\", \n",
    "           \"max(branching factors)\", \"mean(branching factors)\",  \"median(branching factors)\", \"q25(branching factors)\", \"q75(branching factors)\", \n",
    "           \"balancedness factor\", \"container counts list\", \"branching factors list\", \"text\"]].to_csv('results/features_times_DDB.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281d0b73-8920-4155-9f77-fc2e6da6ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_extra[[\"bench\", \"query\", \"orig/rewr(mean)\", \"orig/rewr+rewr(mean)\", \"orig mean\", \"rewr mean\", \"rewr mean+rewr\", \"diff rewr-orig\", \n",
    "           \"diff rewr+rewr-orig\", \"#relations\", \"#conditions\", \"#filters\", \"#joins\", 'min(est. cardinality)', 'max(est. cardinality)', \n",
    "           'mean(est. cardinality)', 'q25(est. cardinality)', 'median(est. cardinality)', 'q75(est. cardinality)', \"depth\", \"min(container counts)\", \n",
    "           \"max(container counts)\", \"mean(container counts)\", \"q25(container counts)\", \"median(container counts)\", \"q75(container counts)\", \n",
    "           \"min(branching factors)\", \"max(branching factors)\", \"mean(branching factors)\", \"median(branching factors)\", \"q25(branching factors)\", \n",
    "           \"q75(branching factors)\", \"balancedness factor\", 'list est. cardinality', \"container counts list\", \"branching factors list\",\n",
    "           \"text\"]].to_csv('results/features_times_DDB_extra.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
