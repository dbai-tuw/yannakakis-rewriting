{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e4175a-8e11-4ab2-b5f2-af33b04201ab",
   "metadata": {},
   "source": [
    "# Running queries using JSONS produced by Scala code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe4fae-04cc-4b9c-beb7-510662a4660d",
   "metadata": {},
   "source": [
    "Install and import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fa7d7-3b51-416f-b221-1369637b4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install psycopg2-binary\n",
    "pip install numpy\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6645fd-f261-4197-ab65-bac11ae4cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import csv\n",
    "import multiprocessing\n",
    "import signal\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c950d3-619f-4c9c-9eac-0853eb8242d4",
   "metadata": {},
   "source": [
    "Function for running one query. This means\n",
    "*  run the original query 5 times (after one initial run, which we do not use)\n",
    "*  run the rewritten queries 5 times (after one initial run, which we do not use) and drop the created tables each time\n",
    "*  take the runtimes and calculate mean, median and standard deviation of time for either the original or rewritten query\n",
    "*  compare the runtimes between the original query, the rewritten query and the rewritten query + the rewriting time (how long the Scala took)\n",
    "*  save everything in a csv output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33ec17-289d-4a51-97d7-3044afcd9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to handle the timeouts\n",
    "def handler_orig(signum, frame):\n",
    "    global timeout_flag_orig\n",
    "    timeout_flag_orig = True\n",
    "    raise Exception(\"Query execution of the original query > 100s\")\n",
    "\n",
    "def handler_rewr(signum, frame):\n",
    "    global timeout_flag_rewr\n",
    "    timeout_flag_rewr = True\n",
    "    raise Exception(\"Query execution of the rewritten query > 100s\")\n",
    "\n",
    "# function to run the query 6 times checking for TO and calculate and saving all values\n",
    "def run_query(benchmark, query):\n",
    "    print(benchmark, query)\n",
    "    file_path = f'rewritten/{benchmark}_{query}_output.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # get the original and rewritten query\n",
    "    original_query = json_data[\"original_query\"]\n",
    "    rewritten_query_list = json_data[\"rewritten_query\"]\n",
    "    rewriting_time = json_data[\"time\"]\n",
    "\n",
    "    # get the drop queries\n",
    "    file_path_drop = f'rewritten/{benchmark}_{query}_drop.json'\n",
    "    with open(file_path_drop, 'r') as file:\n",
    "        json_drop = json.load(file)\n",
    "    drop_query_list = json_drop[\"rewritten_query\"]\n",
    "\n",
    "    # connect to PostgreSQL\n",
    "    if benchmark == \"JOB\":\n",
    "        database = \"imdb\"\n",
    "    else:\n",
    "        database = benchmark.lower()\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "    # if the evaluation takes longer than 100sec then break it\n",
    "    global timeout_flag_orig\n",
    "    global timeout_flag_rewr\n",
    "    timeout_flag_orig = False\n",
    "    timeout_flag_rewr = False\n",
    "\n",
    "    print(\"original1\")\n",
    "    # the first run is just a warm up run and to check for the time out\n",
    "    signal.signal(signal.SIGALRM, handler_orig) \n",
    "    signal.alarm(100) #TO at 100 sec, can be changed\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(original_query)\n",
    "        result = cur.fetchall()\n",
    "    except Exception as exc: \n",
    "        print(exc)\n",
    "    signal.alarm(0) \n",
    "\n",
    "    print(\"rewritten1\")\n",
    "    signal.signal(signal.SIGALRM, handler_rewr) \n",
    "    signal.alarm(100) \n",
    "    try:\n",
    "        for rewritten_query in rewritten_query_list:\n",
    "            cur.execute(rewritten_query)\n",
    "            if rewritten_query.startswith(\"SELECT\"):\n",
    "                result1 = cur.fetchall()\n",
    "        for drop_query in drop_query_list:\n",
    "            cur.execute(drop_query)\n",
    "    except Exception as exc: \n",
    "        print(exc)\n",
    "    signal.alarm(0)\n",
    "\n",
    "    print(timeout_flag_orig, timeout_flag_rewr)\n",
    "    # original and rewritten query are TOs\n",
    "    if timeout_flag_orig and timeout_flag_rewr:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "        orig_or_rewr_mean = \"-\"\n",
    "        orig_or_rewr_or_equal = \"-\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"-\"\n",
    "\n",
    "    # original query is a TO and the rewritten not\n",
    "    elif timeout_flag_orig:\n",
    "        orig_mean = \"TO\"\n",
    "        orig_med = \"TO\"\n",
    "        orig_std = \"-\"\n",
    "        list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        list_rewritten = []\n",
    "        print(\"rewritten\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            # execute the rewritten query\n",
    "            start_time_rewritten = time.time()\n",
    "            for rewritten_query in rewritten_query_list:\n",
    "                cur.execute(rewritten_query)\n",
    "            end_time_rewritten = time.time()\n",
    "            rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "            list_rewritten.append(rewritten_time)\n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list:\n",
    "                cur.execute(drop_query)\n",
    "        rewr_mean = np.mean(list_rewritten)\n",
    "        rewr_med = np.median(list_rewritten)\n",
    "        rewr_std = np.std(list_rewritten)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "\n",
    "        orig_or_rewr_mean = \"rewr\"\n",
    "        orig_or_rewr_or_equal = \"rewr\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "\n",
    "    # rewritten query is a TO and the original not\n",
    "    elif timeout_flag_rewr:\n",
    "        list_original = []\n",
    "        print(\"orig\")\n",
    "        for i in range(5):\n",
    "            # execute the original query\n",
    "            start_time_original = time.time()\n",
    "            cur.execute(original_query)\n",
    "            end_time_original = time.time()\n",
    "            original_time = end_time_original - start_time_original\n",
    "            list_original.append(original_time)\n",
    "        orig_mean = np.mean(list_original)\n",
    "        orig_med = np.median(list_original)\n",
    "        orig_std = np.std(list_original)\n",
    "        \n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        orig_or_rewr_mean = \"orig\"\n",
    "        orig_or_rewr_or_equal = \"orig\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"orig\"\n",
    "\n",
    "    # both queries are no TOs\n",
    "    else:\n",
    "        print(result, result1)\n",
    "        list_original = []\n",
    "        list_rewritten = []\n",
    "        # take times for 5 runs (run 2-6) for the original query and the rewritten query\n",
    "        print(\"orig+rewr\")\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            # execute the original query\n",
    "            start_time_original = time.time()\n",
    "            cur.execute(original_query)\n",
    "            end_time_original = time.time()\n",
    "            original_time = end_time_original - start_time_original\n",
    "            list_original.append(original_time)\n",
    "        \n",
    "            # execute the rewritten query\n",
    "            start_time_rewritten = time.time()\n",
    "            for rewritten_query in rewritten_query_list:\n",
    "                cur.execute(rewritten_query)\n",
    "            end_time_rewritten = time.time()\n",
    "            rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "            list_rewritten.append(rewritten_time)\n",
    "            \n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list:\n",
    "                cur.execute(drop_query)\n",
    "            \n",
    "        orig_mean = np.mean(list_original)\n",
    "        orig_med = np.median(list_original)\n",
    "        orig_std = np.std(list_original)\n",
    "        rewr_mean = np.mean(list_rewritten)\n",
    "        rewr_med = np.median(list_rewritten)\n",
    "        rewr_std = np.std(list_rewritten)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "        if orig_mean > rewr_mean:\n",
    "            orig_or_rewr_mean = \"rewr\"\n",
    "        else:\n",
    "            orig_or_rewr_mean = \"orig\"\n",
    "        if abs(rewr_mean-orig_mean) < 0.05:\n",
    "            orig_or_rewr_or_equal = \"equal 0.05\"\n",
    "        else:\n",
    "            orig_or_rewr_or_equal = orig_or_rewr_mean\n",
    "        if orig_mean > rewr_mean_plus_rewr:\n",
    "            orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "        else:\n",
    "            orig_or_rewr_plus_rewr_mean = \"orig\"\n",
    "\n",
    "    list_output = [benchmark, query] + [orig_mean, rewr_mean, rewr_mean_plus_rewr, orig_or_rewr_mean, orig_or_rewr_or_equal, \\\n",
    "                                        orig_or_rewr_plus_rewr_mean, rewriting_time] + \\\n",
    "                    list_original + [orig_med, orig_std] + list_rewritten + [rewr_med, rewr_std, rewr_med_plus_rewr]\n",
    "    #print(list_output)\n",
    "    file_path = \"results/POS_Scala_comparison_TO_augment_server.csv\"\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d1f13-9ad2-4e38-acfb-6e2e81c220c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_rewritten(benchmark, query):\n",
    "    file_path = f'rewritten/{benchmark}_{query}_output.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    print(query)\n",
    "\n",
    "    rewritten_query_list = json_data[\"rewritten_query\"]\n",
    "    rewriting_time = json_data[\"time\"]\n",
    "\n",
    "    file_path_drop = f'rewritten/{benchmark}_{query}_drop.json'\n",
    "    with open(file_path_drop, 'r') as file:\n",
    "        json_drop = json.load(file)\n",
    "    drop_query_list = json_drop[\"rewritten_query\"]\n",
    "\n",
    "    if benchmark == \"JOB\":\n",
    "        database = \"imdb\"\n",
    "    else:\n",
    "        database = benchmark.lower()\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "        host=\"postgres\",\n",
    "        database=database,\n",
    "        user=database,\n",
    "        password=database\n",
    "    )\n",
    "\n",
    "    # if the evaluation takes longer than 30min then break it\n",
    "    global timeout_flag_rewr\n",
    "    timeout_flag_rewr = False\n",
    "\n",
    "    signal.signal(signal.SIGALRM, handler_rewr) \n",
    "    signal.alarm(100) \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        for rewritten_query in rewritten_query_list:\n",
    "            cur.execute(rewritten_query)\n",
    "            if rewritten_query.startswith(\"SELECT\"):\n",
    "                result1 = cur.fetchall()\n",
    "        for drop_query in drop_query_list:\n",
    "            cur.execute(drop_query)\n",
    "    except Exception as exc: \n",
    "        print(exc)\n",
    "    signal.alarm(0)\n",
    "\n",
    "    # rewritten query is a TO and the original not\n",
    "    if timeout_flag_rewr:\n",
    "        rewr_mean = \"TO\"\n",
    "        rewr_med = \"TO\"\n",
    "        rewr_std = \"-\"\n",
    "        rewr_mean_plus_rewr = \"TO\"\n",
    "        rewr_med_plus_rewr = \"TO\"\n",
    "        list_rewritten = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "\n",
    "        orig_or_rewr_mean = \"-\"\n",
    "        orig_or_rewr_or_equal = \"-\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"-\"\n",
    "\n",
    "    # rewritten query is not a TO\n",
    "    else:\n",
    "        list_rewritten = []\n",
    "        # take times for 5 runs (run 2-6) for the original query and the rewritten query\n",
    "        for i in range(5):\n",
    "            print(i)\n",
    "            # execute the rewritten query\n",
    "            start_time_rewritten = time.time()\n",
    "            for rewritten_query in rewritten_query_list:\n",
    "                cur.execute(rewritten_query)\n",
    "            end_time_rewritten = time.time()\n",
    "            rewritten_time = end_time_rewritten - start_time_rewritten\n",
    "            list_rewritten.append(rewritten_time)\n",
    "            \n",
    "            # drop all created tables\n",
    "            for drop_query in drop_query_list:\n",
    "                cur.execute(drop_query)\n",
    "            \n",
    "        rewr_mean = np.mean(list_rewritten)\n",
    "        rewr_med = np.median(list_rewritten)\n",
    "        rewr_std = np.std(list_rewritten)\n",
    "        rewr_mean_plus_rewr = rewr_mean + rewriting_time\n",
    "        rewr_med_plus_rewr = rewr_med + rewriting_time\n",
    "        orig_or_rewr_mean = \"rewr\"\n",
    "        orig_or_rewr_or_equal = \"rewr\"\n",
    "        orig_or_rewr_plus_rewr_mean = \"rewr\"\n",
    "\n",
    "    orig_mean = \"TO\"\n",
    "    orig_med = \"TO\"\n",
    "    orig_std = \"-\"\n",
    "    list_original = [\"-\", \"-\", \"-\", \"-\", \"-\"]\n",
    "        \n",
    "    list_output = [benchmark, query] + [orig_mean, rewr_mean, rewr_mean_plus_rewr, orig_or_rewr_mean, orig_or_rewr_or_equal, \\\n",
    "                                        orig_or_rewr_plus_rewr_mean, rewriting_time] + \\\n",
    "                    list_original + [orig_med, orig_std] + list_rewritten + [rewr_med, rewr_std, rewr_med_plus_rewr]\n",
    "    #print(list_output)\n",
    "    file_path = \"results/POS_Scala_comparison_TO_augment_server.csv\"\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a37073-1ff5-4bf4-becd-d37a900bef08",
   "metadata": {},
   "source": [
    "Create the output csv with the header. We add the running times for each query then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b905a-be9a-44ee-8c69-e7fb2c8114c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/POS_Scala_comparison_TO_augment_server.csv\"\n",
    "\n",
    "names = [\"bench\", \"query\", \"orig mean\", \"rewr mean\", \"rewr mean+rewr\", \"orig/rewr(mean)\", \"orig/rewr/equal\", \"orig/rewr+rewr(mean)\", \"rewriting\", \n",
    "         \"orig 1\", \"orig 2\", \"orig 3\", \"orig 4\", \"orig 5\", \"orig med\", \"orig_std\", \"rewr 1\", \"rewr 2\", \"rewr 3\", \"rewr 4\", \"rewr 5\", \"rewr med\", \n",
    "         \"rewr_std\", \"rewr med+rewr\", ]\n",
    "\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d8f12-ef70-40db-9938-3e88f516f1ff",
   "metadata": {},
   "source": [
    "### STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c56465-ff3a-4cd9-8512-38189c37d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('STATS')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    number = int(file_split[1].split(\"-\")[0])\n",
    "    if number <= 50:\n",
    "        run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c5474-53d8-40f9-b7fb-a543f1248715",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('STATS')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    number = int(file_split[1].split(\"-\")[0])\n",
    "    if number > 50 and number <= 100:\n",
    "        run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2daf767-e94e-4711-b619-5ee3e31e0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('STATS')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    number = int(file_split[1].split(\"-\")[0])\n",
    "    if number > 100:\n",
    "        run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c61cf-476a-4966-a227-21a756650f85",
   "metadata": {},
   "source": [
    "### SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0b916-8adc-4078-a751-9084a3066a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][0:16]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071411ab-5a3d-4961-9f73-e52338a75a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][15:61]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb558f58-e417-4af6-9323-da32e64f1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][61:64]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc2ad5-fd84-4f65-9e8f-dfe2af707903",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][64:122]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063e124-c3aa-4e70-b1f7-f00b16f57ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][122:129]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fefdf-40b7-4b56-a7b3-e3f53abc7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('SNAP')][129:]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc3cb4-07a3-4e77-a8f0-e4142c5b3a74",
   "metadata": {},
   "source": [
    "### JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067b0d9-6c73-4590-a8b3-aaf883a883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('JOB')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e89aa0-2e25-46d7-90e0-300f131e53ee",
   "metadata": {},
   "source": [
    "### LSQB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc16d52-b608-48ba-a9d0-11f71ae21c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('LSQB')][:10]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b02c5f-0e4b-4ee1-996d-f9697334f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('LSQB')][10:]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query_rewritten(file_split[0], file_split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665d6b1-e165-43a5-ad64-0c92fcb6e92b",
   "metadata": {},
   "source": [
    "### HETIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06cc75-585e-4848-8826-e35a80c888cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'rewritten/'\n",
    "files = sorted(os.listdir(folder_path))\n",
    "output_files = [file for file in files if file.endswith('_output.json') and file.startswith('HETIO')]\n",
    "\n",
    "for file in output_files:\n",
    "    file_split = file.split(\"_\")\n",
    "    run_query(file_split[0], file_split[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
